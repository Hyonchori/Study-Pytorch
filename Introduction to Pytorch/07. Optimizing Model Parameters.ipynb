{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17127016",
   "metadata": {},
   "source": [
    "# OPTIMIZING MODEL PARAMETERS\n",
    "- 모델과 데이터가 준비됐다면 이제 학습할 차례\n",
    "- 모델을 학습하는 작업은 **반복적인 과정(epoch)** 임\n",
    ">1. 모델의 예측값 출력\n",
    ">2. loss function 으로 정답값에 대한 error 계산\n",
    ">3. error 를 모델의 파라미터로 편미분하여 derivative 구함\n",
    ">**4. 미분 결과를 이용해 파라미터를 optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb31ff",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Prerequisite Code\n",
    "- **Datasets & DataLoader** 와 **Build Model** 부분을 작성한 뒤에 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "495e0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92bf7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eca1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb35a6",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Hyperparameters\n",
    "- **하이퍼파라미터란** 모델의 최적화 과정에 관여하는 조절 가능한 파라미터를 뜻함\n",
    "- 하이퍼파라미터가 달라지면 모델의 학습의 수렴에 영향을 줄 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5026889",
   "metadata": {},
   "source": [
    ">여기선 다음 세 가지 하이퍼파라미터를 다뤄봄\n",
    ">1. **Number of Epochs** : 데이터셋을 iterate 할 횟수\n",
    ">2. **Batch size** : 네트워크를 업데이트하기 위해 propagation 에 사용될 데이터 샘플 수\n",
    ">3. **Learning rate** : 각 batch/epoch 마다 모델의 파라미터를 업데이트 하는 정도 (작으면 학습 과정이 길어지고, 크면 발산할 위험이 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fa0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size =64\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09b485",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Optimization Loop\n",
    "- 하이퍼파라미터를 설정했다면 이제 **optimization loop** 를 만들 차례\n",
    "- **학습의 각 iteration(epoch) 을 정의하는 것**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19049bc6",
   "metadata": {},
   "source": [
    ">각 에폭은 두 개의 메인 파트로 이루어짐\n",
    ">1. **The Train Loop** : 학습 데이터셋에 대한 iteration. **파라미터의 최적화가 이뤄짐**\n",
    ">2. **The Validation/Test Loop** : 테스트셋에 대한 iteration. **모델의 성능을 확인함**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7453dd1",
   "metadata": {},
   "source": [
    "## 3.1 Loss Function\n",
    "- 초기의 네트워크는 학습 데이터를 통해 올바른 출력을 하기 어려움\n",
    "- **Loss Function** 은 모델이 얼마나 출력을 못했는지 측정해줌\n",
    "- 이 loss function 을 최소화하는 것이 바로 학습임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02074da",
   "metadata": {},
   "source": [
    ">- **nn.MSELoss(Mean Square Error)** : regression 작업에 주로 사용하는 loss\n",
    ">- **nn.NLLLoss(Negative Log Likelihood)** : classification 작업에 주로 사용하는 loss\n",
    ">- **nn.CrossEntropyLoss** : ```nn.LogSoftmax``` 와 ```nn.NLLLoss``` 를 합친 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9914cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f549107",
   "metadata": {},
   "source": [
    "## 3.2 Optimizer\n",
    "- 최적화란 모델의 파라미터를 loss function 을 줄이는 쪽으로 업데이트하는 작업임\n",
    "- **Optimization algorithms** 은 이 과정을 어떻게 수행할 것인지 정의함\n",
    "- 여기선 **SGD(stochastic gradient descent)** 를 사용함\n",
    "- 파이토치는 **ADAM, RMSProp** 같은 다양한 optimizer 를 제공함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9b8a3",
   "metadata": {},
   "source": [
    "- **optimizer 를 초기화할 땐 최적화할 모델의 파라미터를 등록해야 함**\n",
    "- learning rate 도 설정해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41f6a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda495be",
   "metadata": {},
   "source": [
    ">**1 학습 루프 당 진행되는 3 단계**\n",
    ">1. ```optimizer.zero_grad()``` 를 호출하여 파라미터들의 gradient 를 초기화.\n",
    ">2. ```loss.backward()``` 를 호출하여 에러를 역전파. 이때 각 파라미터에 대한 gradient 를 구함\n",
    ">3. ```optimizer.step()``` 를 호출하여 파라미터를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab47e5",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Full Implementation\n",
    "- 모델 학습을 위한 ```train_loop```\n",
    "- 모델 평가를 위한 ```test_loop``` 를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb54f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch%100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}   [{current:>5d} / {size:>5d}]\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ace70",
   "metadata": {},
   "source": [
    "- loss function 과 optimizer 를 정의했다면 ```train_loop``` 와 ```test_loop``` 에 전달하여 학습 및 평가를 진행\n",
    "- 성능 향상을 위해 epoch 을 높여볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df03a6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 2.262627   [    0 / 60000]\n",
      "loss: 2.262111   [ 6400 / 60000]\n",
      "loss: 2.245964   [12800 / 60000]\n",
      "loss: 2.230340   [19200 / 60000]\n",
      "loss: 2.228112   [25600 / 60000]\n",
      "loss: 2.198032   [32000 / 60000]\n",
      "loss: 2.210687   [38400 / 60000]\n",
      "loss: 2.200469   [44800 / 60000]\n",
      "loss: 2.182735   [51200 / 60000]\n",
      "loss: 2.139715   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 0.033990 \n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "loss: 2.196808   [    0 / 60000]\n",
      "loss: 2.195006   [ 6400 / 60000]\n",
      "loss: 2.181229   [12800 / 60000]\n",
      "loss: 2.153560   [19200 / 60000]\n",
      "loss: 2.139691   [25600 / 60000]\n",
      "loss: 2.108427   [32000 / 60000]\n",
      "loss: 2.114838   [38400 / 60000]\n",
      "loss: 2.113174   [44800 / 60000]\n",
      "loss: 2.079001   [51200 / 60000]\n",
      "loss: 2.013290   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 0.032367 \n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "loss: 2.115090   [    0 / 60000]\n",
      "loss: 2.108305   [ 6400 / 60000]\n",
      "loss: 2.100135   [12800 / 60000]\n",
      "loss: 2.053534   [19200 / 60000]\n",
      "loss: 2.031322   [25600 / 60000]\n",
      "loss: 2.009829   [32000 / 60000]\n",
      "loss: 1.997267   [38400 / 60000]\n",
      "loss: 2.018813   [44800 / 60000]\n",
      "loss: 1.961352   [51200 / 60000]\n",
      "loss: 1.876395   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 0.030635 \n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "loss: 2.023222   [    0 / 60000]\n",
      "loss: 2.014908   [ 6400 / 60000]\n",
      "loss: 2.013679   [12800 / 60000]\n",
      "loss: 1.953866   [19200 / 60000]\n",
      "loss: 1.922358   [25600 / 60000]\n",
      "loss: 1.913546   [32000 / 60000]\n",
      "loss: 1.888720   [38400 / 60000]\n",
      "loss: 1.933456   [44800 / 60000]\n",
      "loss: 1.855251   [51200 / 60000]\n",
      "loss: 1.763328   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 0.029112 \n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "loss: 1.936163   [    0 / 60000]\n",
      "loss: 1.930183   [ 6400 / 60000]\n",
      "loss: 1.931761   [12800 / 60000]\n",
      "loss: 1.871475   [19200 / 60000]\n",
      "loss: 1.827613   [25600 / 60000]\n",
      "loss: 1.822559   [32000 / 60000]\n",
      "loss: 1.802716   [38400 / 60000]\n",
      "loss: 1.862990   [44800 / 60000]\n",
      "loss: 1.772320   [51200 / 60000]\n",
      "loss: 1.678159   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 0.027883 \n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "loss: 1.863201   [    0 / 60000]\n",
      "loss: 1.858130   [ 6400 / 60000]\n",
      "loss: 1.859432   [12800 / 60000]\n",
      "loss: 1.806065   [19200 / 60000]\n",
      "loss: 1.749614   [25600 / 60000]\n",
      "loss: 1.743156   [32000 / 60000]\n",
      "loss: 1.734017   [38400 / 60000]\n",
      "loss: 1.804441   [44800 / 60000]\n",
      "loss: 1.707472   [51200 / 60000]\n",
      "loss: 1.615351   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 0.026887 \n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "loss: 1.800842   [    0 / 60000]\n",
      "loss: 1.797766   [ 6400 / 60000]\n",
      "loss: 1.797014   [12800 / 60000]\n",
      "loss: 1.753955   [19200 / 60000]\n",
      "loss: 1.687498   [25600 / 60000]\n",
      "loss: 1.677266   [32000 / 60000]\n",
      "loss: 1.678396   [38400 / 60000]\n",
      "loss: 1.754810   [44800 / 60000]\n",
      "loss: 1.655262   [51200 / 60000]\n",
      "loss: 1.571622   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 0.026102 \n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "loss: 1.749212   [    0 / 60000]\n",
      "loss: 1.749954   [ 6400 / 60000]\n",
      "loss: 1.746410   [12800 / 60000]\n",
      "loss: 1.713514   [19200 / 60000]\n",
      "loss: 1.639126   [25600 / 60000]\n",
      "loss: 1.626391   [32000 / 60000]\n",
      "loss: 1.634850   [38400 / 60000]\n",
      "loss: 1.716526   [44800 / 60000]\n",
      "loss: 1.614542   [51200 / 60000]\n",
      "loss: 1.542404   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 0.025505 \n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "loss: 1.706401   [    0 / 60000]\n",
      "loss: 1.714793   [ 6400 / 60000]\n",
      "loss: 1.706785   [12800 / 60000]\n",
      "loss: 1.681328   [19200 / 60000]\n",
      "loss: 1.601730   [25600 / 60000]\n",
      "loss: 1.586194   [32000 / 60000]\n",
      "loss: 1.599820   [38400 / 60000]\n",
      "loss: 1.687481   [44800 / 60000]\n",
      "loss: 1.580868   [51200 / 60000]\n",
      "loss: 1.520518   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 0.025037 \n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "loss: 1.668997   [    0 / 60000]\n",
      "loss: 1.686771   [ 6400 / 60000]\n",
      "loss: 1.674857   [12800 / 60000]\n",
      "loss: 1.656027   [19200 / 60000]\n",
      "loss: 1.572968   [25600 / 60000]\n",
      "loss: 1.554727   [32000 / 60000]\n",
      "loss: 1.570253   [38400 / 60000]\n",
      "loss: 1.663301   [44800 / 60000]\n",
      "loss: 1.551926   [51200 / 60000]\n",
      "loss: 1.502977   [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 0.024650 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8531701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
