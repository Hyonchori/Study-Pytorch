{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca39edb",
   "metadata": {},
   "source": [
    "# SAVE AND LOAD THE MODEL\n",
    "- 이 섹션에선 모델의 저장, 로드, 동작과 함께 모델을 지속할 수 있는 방법에 대해 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673328fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef96f9e",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Saving and Loading Model Weights\n",
    "- 파이토치의 모델은 ```state_dict``` 라는 내부 state dictionary에 학습된 파라미터를 보관함\n",
    "- 파라미터는 ```torch.save``` 메서드를 이용해 저장할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a15e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\gus8c/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e7ef19c3e8493f9fe98fa888a00cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'my_vgg16.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9667a",
   "metadata": {},
   "source": [
    ">저장한 모델의 weight 를 로드하기 위해선\n",
    ">1. 동일한 모델의 인스턴스를 생성\n",
    ">2. ```load_state_dict()``` 메서드로 새 인스턴스에 저장한 weight 를 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad15f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16()\n",
    "model.load_state_dict(torch.load(\"my_vgg16.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d7cae",
   "metadata": {},
   "source": [
    ">**model.eval()**\n",
    ">- 모델 내부에 있는 **학습 때와 평가때의 동작 방식이 다른 layer** 들을 평가모드로 바꾸는 부분\n",
    ">- **dropout, batchnormalization** 등이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d970fc",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Saving and Loading Models with Shapes\n",
    "- 모델의 weight 를 로드할 때, 네트워크의 구조를 정의하기 위해 우선 인스턴스를 생성해야 했음\n",
    "- 파이토치는 **네트워크의 구조와 파라미터를 한 번에 같이 저장할 수도 있음**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48281102",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"my_vgg16_with_class.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f11df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"my_vgg16_with_class.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587328bf",
   "metadata": {},
   "source": [
    ">- 이 접근 방식은 파이썬의 **pickle** 모듈을 이용하여 모델을 시리얼라이즈함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3a6ce",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Exporting Model to ONNX\n",
    "- 파이토치는 **ONNX** 를 지원함\n",
    "- 파이토치는 특성상 export 작업을 할 때 ONNX 모델을 생성하기 위해 execution graph 를 거쳐야 함\n",
    "- 때문에 export 할 때 **적절한 size 가 주어져야 함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f628414",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.zeros((1, 3, 224, 224))\n",
    "onnx.export(model, input_image, \"my_vgg16.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb1eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
