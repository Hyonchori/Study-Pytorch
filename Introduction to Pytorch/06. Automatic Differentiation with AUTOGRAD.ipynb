{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be0550a",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION WITH ```torch.autograd```\n",
    "- **Back propagation** 은 인공신경망을 학습할 때 가장 많이 사용되는 알고리즘임\n",
    "- 역전파에서 모델의 파라미터는 loss function 에 대한 **gradient** 에 따라 조정됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aba5e0",
   "metadata": {},
   "source": [
    "- 파이토치는 ```torch.autograd``` 라는 내장 미분 엔진을 이용하여 gradient 를 계산함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bc57e",
   "metadata": {},
   "source": [
    "- 1층으로 구성된 간단한 인공신경망을 예로 들겠음\n",
    "- 입력 ```x```, 파라미터 ```w``` 와 ```b```, loss function 을 정의해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6e0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad = True)\n",
    "b = torch.randn(3, requires_grad = True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4747583",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Tensors, Functions and Computational graph\n",
    "- 위 모델에서 최적화할 파라미터는 ```w, b``` 임\n",
    "- 그러기 위해선 loss function 에 대한 각 변수의 gradient 를 계산해야 함\n",
    "- 이를 위해 파라미터 텐서의 ```requires_grad``` 특성을 설정함\n",
    "\n",
    ">변수의 ```requires_grad``` 는 ```x.requires_grad_(True)``` 형식으로도 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98d8be",
   "metadata": {},
   "source": [
    "- backward propagation 에 참조될 함수 객체는 텐서의 ```grad_fn``` 특성에 저장돼 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b164c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z =  <AddBackward0 object at 0x00000278FB8B8BE0>\n",
      "Gradient function for loss =  <BinaryCrossEntropyWithLogitsBackward object at 0x00000278FB8B8B20>\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient function for z = \", z.grad_fn)\n",
    "print(\"Gradient function for loss = \", loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8af01",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Compution Gradients\n",
    "- weight 파라미터를 최적화하기 위해선 각 파라미터로 loss function 을 편미분하여 derivative 를 구해야 함\n",
    "- ```loss.backward()``` 를 호출하여 이 derivative 를 구할 수 있음\n",
    "- 호출한 뒤 ```w.grad, b.grad``` 에서 해당 정보를 확인 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe157f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0045, 0.3152, 0.2234],\n",
      "        [0.0045, 0.3152, 0.2234],\n",
      "        [0.0045, 0.3152, 0.2234],\n",
      "        [0.0045, 0.3152, 0.2234],\n",
      "        [0.0045, 0.3152, 0.2234]])\n",
      "tensor([0.0045, 0.3152, 0.2234])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38bc63",
   "metadata": {},
   "source": [
    ">- 이때 ```grad``` 특성을 얻을 수 있는 노드는 ```requires_grad=True``` 로 설정된 노드들 뿐임\n",
    ">- 주어진 그래프에 대해 ```backward```는 **오직 한 번만 호출 가능**\n",
    ">- 같은 그래프에 대해 여러 번 호출이 필요한 경우엔 ```backward``` 호출 시 ```retain_graph=True``` 설정해야 함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44bb0f",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Disabling Gradient Tacking\n",
    "- ```requires_grad=True``` 로 설정된 텐서들은 기본적으로 자신의 연산 기록을 계속 추적함\n",
    "- 그러나 가끔은 그럴 필요가 없음 (그냥 입력 데이터를 처리해볼 경우)\n",
    "- ```torch.no_grad()``` 블록을 이용하여 연산 기록의 추적을 멈출 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ca9330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ad611",
   "metadata": {},
   "source": [
    "- 다른 방법으로는 텐서의 ```datach()``` 메서드를 사용하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f34919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02db89",
   "metadata": {},
   "source": [
    "> **gradient tracking** 을 중단하는 몇몇 이유\n",
    ">- 모델의 파라미터를 **frozen parameters** 로 만들기 위해 (주로 사전학습된 네트워크를 파인튜닝할 때)\n",
    ">- forward 연산을 할 때 **연산 속도를 높이기 위해**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5a15a",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. More on Computational Graphs\n",
    "- 콘셉상으로 autograd 는 **DAG(directed acyclic graph)** 내에서 만들어진 데이터(텐서)와 실행된 모든 연산을 기록함\n",
    "- DAG 는 **Function** 객체로 구성됨\n",
    "- DAG 에서 입력 텐서는 leaves, 출력 텐서는 roots 가 됨\n",
    "- roots 에서부터 leaves 까지 추적하여 **chain rule** 에 따라 gradient 를 자동으로 계산함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b8e57",
   "metadata": {},
   "source": [
    ">**forward 에서 autograd 가 동시에 수행하는 두 가지 작업**\n",
    ">- 결과 텐서를 계산하기 위해 필요한 연산\n",
    ">- 각 연산의 **gradient function** 을 DAG 에서 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436231f3",
   "metadata": {},
   "source": [
    ">**DAG 에서 ```.backward()``` 가 호출될 때 autograd 가 수행하는 작업**\n",
    ">- 각 ```.grad_fn```에 대해 gradient 계산\n",
    ">- 각 텐서의 ```.grad``` 특성에 gradinet 를 축적\n",
    ">- chaing rule 에 따라 leaf 에 해당하는 텐서들에 모두 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15f2a8",
   "metadata": {},
   "source": [
    ">- ```.backward()``` 를 아무런 파라미터 없이 호출하는 것은 ```.backward(torch.tensor(1.0))``` 과 동일함\n",
    ">- 이는 **scalar-valued function*** 의 그레디언트를 구할 때 유용한 방법이라고 함 (즉 모델의 손실 함수)\n",
    ">- **만약 그레디언트를 구할 대상이 스칼라가 아니라면 별도의 파라미터 필요**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5024d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
