{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27785b7",
   "metadata": {},
   "source": [
    "# A GENTLE INTRODUCTION TO ```torch.autograd```\n",
    "- ```torch.autograd``` 는 인공신경망 학습을 위한 파이토치의 자동 미분 엔진임\n",
    "- 이 섹션에서는 **autograd 가 어떻게 인공신경망의 학습을 돕는지에 대한 컨셉을 이해해볼 것**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f75fc",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Background\n",
    "- 인공신경망은 입력 함수에 대해 적용되는 밀집 함수의 집합임\n",
    "- 이 함수는 weight 와 bias 로 이루어진 **파라미터로 정의됨**\n",
    "- 이 파라미터는 파이토치에서 텐서의 형태로 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994c500",
   "metadata": {},
   "source": [
    ">**인공신경망의 학습은 두 단계로 나뉨**\n",
    ">1. **Forward Propagation** : 인공신경망은 정답값에 대한 최선의 출력을 만듦. 입력을 받으면 여러 함수를 거쳐 최종 출력을 반환함\n",
    ">2. **Backward Propagation** : 위 단계에서 발생한 에러에 비례하여 파라미터를 조정함. 출력으로부터 입력단계까지 역전파해가며 각 파라미터에 대한 derivatives 를 구함. 구한 derivatives 를 기반으로한 gradient descent 를 이용하여 파라미터를 조정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645a9dd",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Usage in PyTorch\n",
    "- 간단한 1 strp 학습을 예로 들겠음\n",
    "- 먼저 ```torchvision``` 을 통해 사전학습된 resnet18 모델을 로드\n",
    "- 해당 모델의 입력을 임의로 생성 (채널 3, 가로 세로 64) / (라벨 임의의 정수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3f810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to C:\\Users\\gus8c/.cache\\torch\\hub\\checkpoints\\resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2fe8090c804ebebf82aba6bcad3926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af75293",
   "metadata": {},
   "source": [
    "- 이제 임의로 생성한 입력을 모델에 넣어 출력을 만들어 보겠음\n",
    "- 이 작업은 **forward pass** 로 진행됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9af2b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "pred = model(data)\n",
    "print(pred.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f352bf",
   "metadata": {},
   "source": [
    "- 모델의 예측과 정답값을 이용하여 에러(```loss```)를 구함\n",
    "- 에러를 구하면 네트워크에 **역전파** 함. 이는 에러 텐서의 ```.backward()``` 메서드를 호출하면 됨\n",
    "- **Autograd** 는 모델의 각 파라미터의 ```.grad``` 특성에 gradient 를 구하여 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20013f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (pred - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca7634",
   "metadata": {},
   "source": [
    "- 이제 **optimizer** 를 정의하여 학습할 파라미터를 로드함\n",
    "- 여기선 **SGD** 를 사용. **학습률**은 0.01, **모멘텀**은 0.9 로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28493be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0223a",
   "metadata": {},
   "source": [
    "- 마지막으로 ```.step()``` 메서드를 호출하여 gradient descent 를 실행함\n",
    "- 호출하면 optimizer 는 각 파라미터의 ```.grad``` 특성에 저장된 gradient 를 이용하여 업데이트함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0157b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3ac97",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Difference in Autograd\n",
    "- ```autograd``` 가 어떻게 gradient 를 수집하는지 보겠음\n",
    "- 예시로 ```requires_grea=True``` 로 설정한 두 텐서를 만듦. 이제 이 두 텐서가 사용되는 모든 연산은 ```autograd``` 가 추적할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f40ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5387913",
   "metadata": {},
   "source": [
    "- 위에서 만든 ```a, b``` 로 이루어진 새로운 텐서 ```Q``` 를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7b26d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea08e6",
   "metadata": {},
   "source": [
    ">```a, b``` 를 인공신경망의 파라미터, ```Q``` 를 그 신경망의 에러라고 가정\n",
    ">- ```Q``` 에 대해 ```.backward()```를 호출하면 사용된 파라미터인 ```a, b``` 의 gradient 가 구해짐\n",
    ">- 이 gradient 는 각 파라미터의 ```.grad()``` 특성에 저장됨\n",
    ">- **이때 에러가 1차원이 아닌 벡터이므로 ```Q.backward()``` 메서드에 ```gradient``` 매개변수를 전달해야 함**. ```gradient``` 매개변수엔 ```Q``` 와 같은 shape 를 가진 텐서를 넣으면 됌. **이는 Q 자기 자신에 대한 gradient 와 같음**\n",
    ">- ```Q.sum().backward()``` 형식으로 벡터를 스칼라로 바꿔서 진행할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d301702",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22581d3",
   "metadata": {},
   "source": [
    "- 역전파로 각 파라미터에 대한 gradient 가 계산됨\n",
    "- 각 파라미터의 ```.grad``` 특성을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13883f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e041aa",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Exclusion from the DAG\n",
    "- ```torch.autograd``` 는 ```requires_grad=True``` 로 설정된 모든 텐서들이 관여한 연산을 추적함\n",
    "- 추적이 필요하지 않은 텐서는 ```requires_grad=False``` 로 설정하면 됨. 그러면 gradient computation DAG 로부터 배제됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be4f1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = x + z\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b4716",
   "metadata": {},
   "source": [
    "## 4.1 Frozen Parameters\n",
    "- 인공신경망에서 gradient 를 계산하지 않는 파라미터를 **frozen parameters** 라고 함\n",
    "- 특히 사전학습된 모델을 사용할 때 모델의 일부분을 **freeze** 하는 것은 연산을 줄일 수 있어 효율적임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e84f7",
   "metadata": {},
   "source": [
    ">또는 **사전학습된 모델을 파인튜닝** 할 때 매우 유용함\n",
    ">- 최종 출력층을 제외한 모든 층을 freeze 하고\n",
    ">- 마지막 출력층만 따로 학습하는 식으로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cab14cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813f8cf",
   "metadata": {},
   "source": [
    "- 새롭게 분류할 타겟이 **10개의 클래스**를 갖고 있다면 출력이 10개가 되야함\n",
    "- 여기서 사용한 모델은 출력층이 1000 개임\n",
    "- 여기선 모델의 출력층인 ```model.fc``` 층을 수정하여 구현하겠음 (기본적으로 unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33a4418d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7bf5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309da79",
   "metadata": {},
   "source": [
    "- 이제 모델은 출력층을 제외하고 모든 파라미터가 frozen 상태임\n",
    "- 에러를 구하고 gradient 를 구하면 출력층에 대해서만 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03eed0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97002856",
   "metadata": {},
   "source": [
    ">- 여기서 모든 파라미터를 optimizer 에 올렸지만 gradient 가 구해지는 파라미터는 unfrozen 파라미터 뿐이므로 안심"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0059d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
