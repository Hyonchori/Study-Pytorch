{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b05e11c",
   "metadata": {},
   "source": [
    "# LEARNING PYTORCH WITH EXAMPLES\n",
    "- 해당 튜토리얼은 다양한 예시를 통해 **파이토치** 의 핵심 개념을 소개함\n",
    ">**파이토치의 두 가지 핵심 특징**\n",
    ">1. numpy 의 array 와 흡사한 n-차원 **텐서**. 텐서는 GPU 상에서 동작할 수 있음\n",
    ">2. 인공신경망을 설계하고 학습하기 위한 **Automatic differentitaion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea319f",
   "metadata": {},
   "source": [
    "- **여기선 ```y = sin(x)``` 를 3차 다항식으로 fitting 하는 문제를 예시로 듬**\n",
    "- 네트워크는 4개의 파라미터를 가지며 유클라디안 거리를 최소화하는 방식으로 gradient descent 를 통해 최적화할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ad2e4",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6bbb3",
   "metadata": {},
   "source": [
    "## 1.1 Warm-up: numpy\n",
    "- 파이토치로 바로 구현하기 앞서 numpy 배열을 이용한 구현을 먼저 해보겠음\n",
    "- numpy 는 연산 그래프나 딥 러닝에 대한 어떤 것도 적용되어 있지 않지만 그래도 해당 문제에 적용은 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3964448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1813.5364278327934\n",
      "199 1273.72760597244\n",
      "299 895.866082742301\n",
      "399 631.2046172607264\n",
      "499 445.72355660426643\n",
      "599 315.6624659912412\n",
      "699 224.41461776209417\n",
      "799 160.3653631524902\n",
      "899 115.38625175130733\n",
      "999 83.7851296711114\n",
      "1099 61.57358073031236\n",
      "1199 45.95541081337591\n",
      "1299 34.96922284307594\n",
      "1399 27.238492906400417\n",
      "1499 21.79670056661665\n",
      "1599 17.964897471379363\n",
      "1699 15.265938384639387\n",
      "1799 13.364361781069498\n",
      "1899 12.02422655460381\n",
      "1999 11.079526565073337\n",
      "Result: y = -0.04920264078751947 + 0.8469417881331966x + 0.008488274271323823x^2 + -0.09193655880622141x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = a + b*x + c*x**2 + d*x**3\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    \n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x**2).sum()\n",
    "    grad_d = (grad_y_pred * x**3).sum()\n",
    "    \n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f\"Result: y = {a} + {b}x + {c}x^2 + {d}x^3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616b21a",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 PyTorch: Tensors\n",
    "- Numpy 는 매우 훌륭한 프레임워크지만 GPU 를 지원하지 않음\n",
    "- 최신 심층신경망은 GPU 환경에서 수배~수십배 빠른 속도를 보이므로 numpy 는 딥러닝에 적합하지 않음\n",
    "- 그래서 PyTorch 는 **텐서** 를 사용함. 개념상 numpy array 와 동일하지만 인공신경망 학습을 위한 연산 추적 및 다양한 연산 함수를 제공함\n",
    "- 또한 **텐서를 GPU 상에 올려 동작하는 것이 가능**함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318d62b",
   "metadata": {},
   "source": [
    "- 위 예시를 torch 를 이용하여 구현해보겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485f4f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1499.564208984375\n",
      "199 1004.4855346679688\n",
      "299 674.2183837890625\n",
      "399 453.7781982421875\n",
      "499 306.5608825683594\n",
      "599 208.18621826171875\n",
      "699 142.4093017578125\n",
      "799 98.3998031616211\n",
      "899 68.9347915649414\n",
      "999 49.19337463378906\n",
      "1099 35.95713806152344\n",
      "1199 27.07569122314453\n",
      "1299 21.111589431762695\n",
      "1399 17.10318374633789\n",
      "1499 14.406929016113281\n",
      "1599 12.5916748046875\n",
      "1699 11.368440628051758\n",
      "1799 10.543407440185547\n",
      "1899 9.986397743225098\n",
      "1999 9.609949111938477\n",
      "Result: y = -0.018315279856324196 + 0.8351486921310425x + 0.0031596915796399117x^2 + -0.09025909006595612x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = a + b*x + c*x**2 + d*x**3\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    \n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x**2).sum()\n",
    "    grad_d = (grad_y_pred * x**3).sum()\n",
    "    \n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f\"Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd2d37",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a34b7",
   "metadata": {},
   "source": [
    "## 2.1 PyTorch: Tensors and autograd\n",
    "- 위 예씨에선 **forward, backward** 과정을 직접 작성하여 적용함\n",
    "- 직접 구현하는 작업은 네트워크가 작으면 간단하지만 모델이 커질수록 감당하기 어려워짐\n",
    "- 다행히 파이토치가 제공하는 **autograd** 패키지를 통해 backward 연산에서 필요한 자동 미분을 이용할 수 있음\n",
    "- autograd 를 사용하면 네트워크가 **computational graph**를 정의함. 그래프의 노드는 텐서가 되고 엣지는 출력 텐서를 만드는 연산이 됨. 이 그래프에 대한 역전파로 gradient 를 계산하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e10c92",
   "metadata": {},
   "source": [
    "- 설명으론 복잡하지만 실제로 사용하기는 간단함\n",
    "- 그래프를 이루는 텐서의 ```requires_grad=True``` 특성을 설정하면 ```.grad``` 특성에 해당 텐서가 사용된 연산에 대한 gradient 를 축적함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d90c4d",
   "metadata": {},
   "source": [
    "- 이제 위 문제를 autograd 를 이용하여 구현해 보겠음\n",
    "- 더이상 backward 연산을 직접 구현하지 않아도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ab4e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 4690.0546875\n",
      "199 3130.77197265625\n",
      "299 2091.953125\n",
      "399 1399.5548095703125\n",
      "499 937.8284912109375\n",
      "599 629.7680053710938\n",
      "699 424.12115478515625\n",
      "799 286.7630615234375\n",
      "899 194.96263122558594\n",
      "999 133.571533203125\n",
      "1099 92.48971557617188\n",
      "1199 64.97978210449219\n",
      "1299 46.545005798339844\n",
      "1399 34.182437896728516\n",
      "1499 25.88555335998535\n",
      "1599 20.312847137451172\n",
      "1699 16.566728591918945\n",
      "1799 14.046306610107422\n",
      "1899 12.349081039428711\n",
      "1999 11.20508861541748\n",
      "Result: y = 0.029936792328953743 + 0.818003237247467x + -0.0051645925268530846x^2 + -0.08782030642032623x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = a + b*x + c*x**2 + d*x**3\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f\"Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87cb29",
   "metadata": {},
   "source": [
    "## 2.2 PyTorch: Defining new autograd functions\n",
    ">**autograd operator 는 두 가지 함수를 텐서에 적용함**\n",
    ">1. **forward** : 입력 텐서로부터 출력 텐서를 계산\n",
    ">2. **backward** : 출력 텐서의 gradient 를 입력받아 입력 텐서에 대한 gradient 를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebc6ee",
   "metadata": {},
   "source": [
    "- 파이토치에선 ```torch.autograd.Function``` 클래스를 상속하여 **사용자 정의 autograd function** 을 만들 수 있음\n",
    "- 해당 클래스에 ```forward, backward``` 메서드를 구현하면 됨\n",
    "- 클래스의 인스턴스를 만들고 이를 텐서를 입력받는 함수로 호출하는 것으로 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ca830",
   "metadata": {},
   "source": [
    "- 예시로 **Legendre polynomial** 다항식을 구현해 보겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efe69fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_data):\n",
    "        ctx.save_for_backward(input_data)\n",
    "        return 0.5 * (5*input_data**3 - 3*input_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_data, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 *input_data**2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a59aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03520202636719\n",
      "499 50.978515625\n",
      "599 37.40313720703125\n",
      "699 28.20686912536621\n",
      "799 21.973186492919922\n",
      "899 17.745729446411133\n",
      "999 14.877889633178711\n",
      "1099 12.931766510009766\n",
      "1199 11.610918998718262\n",
      "1299 10.714248657226562\n",
      "1399 10.105474472045898\n",
      "1499 9.692106246948242\n",
      "1599 9.411375045776367\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = -1.765793067320942e-11 + -2.208526849746704*P3(9.924167737596079e-11 + 0.2554861009120941x)\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    y_pred = a + b*P3(c + d*x)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        \n",
    "print(f\"Result: y = {a.item()} + {b.item()}*P3({c.item()} + {d.item()}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633999c9",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ```nn.module```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aadfc5",
   "metadata": {},
   "source": [
    "## 3.1 PyTorch: nn\n",
    "- computational graph 와 autograd 는 복잡한 연산의 정의와 derivatives 자동 계산에 매우 강력함\n",
    "- 하지만 매우 큰 네트워크에 대해서 raw autograd 는 너무 low-level 임\n",
    "- 네트워크를 구축할 때 우리는 **학습 가능한 파라미터로 이루어진 레이어에 연산을 구현함**\n",
    "- TensorFlow, Keras, TFLearn 같은 패키지는 네트워크를 간편하게 구축할 수 있도록 다양한 high-level 레이어를 제공함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb2405",
   "metadata": {},
   "source": [
    "- 파이토치에선 ```nn``` 패키지가 그 역할을 해줌\n",
    "- ```nn``` 패키지는 네트워크의 레이어와 동일한 역할을 하는 **Module** 을 정의함\n",
    "- 모듈은 입력 텐서를 받아 출력 텐서를 계산함.\n",
    "- ```nn``` 패키지는 또한 loss function 도 정의할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9032f",
   "metadata": {},
   "source": [
    "- 위 예시에서 사용한 다항 모델을 ```nn``` 패키지를 이용하여 구현해 보겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b845eb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 853.07666015625\n",
      "199 567.9133911132812\n",
      "299 379.1068420410156\n",
      "399 254.08680725097656\n",
      "499 171.29574584960938\n",
      "599 116.46434020996094\n",
      "699 80.14610290527344\n",
      "799 56.087615966796875\n",
      "899 40.148502349853516\n",
      "999 29.58719825744629\n",
      "1099 22.588254928588867\n",
      "1199 17.94947052001953\n",
      "1299 14.874412536621094\n",
      "1399 12.835579872131348\n",
      "1499 11.483595848083496\n",
      "1599 10.586894035339355\n",
      "1699 9.992032051086426\n",
      "1799 9.597319602966309\n",
      "1899 9.33536148071289\n",
      "1999 9.161462783813477\n",
      "Result: y = 0.005298037081956863 + 0.8393722176551819 x +       -0.0009139998001046479 x^2 + -0.09085985273122787 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \n",
    "linear_layer = model[0]\n",
    "\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + \\\n",
    "      {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c963cd1",
   "metadata": {},
   "source": [
    "## 3.2 PyTorch: optim\n",
    "- 모델의 파라미터를 업데이트할 때 ```torch.no_grad``` 블록 상에서 일일히 진행했음\n",
    "- 이는 stochastic gradient descent 에선 어렵지 않으나 더 상위 알고리즘에선 직접 구현하기가 매우 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c96dc7",
   "metadata": {},
   "source": [
    "- 파이토치는 ```optim``` 패키지를 제공하여 파라미터의 최적화를 간단하게 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8e731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 890.3433227539062\n",
      "199 600.4813232421875\n",
      "299 494.2259826660156\n",
      "399 386.91204833984375\n",
      "499 282.89788818359375\n",
      "599 194.0765380859375\n",
      "699 125.3480224609375\n",
      "799 76.06620788574219\n",
      "899 43.026580810546875\n",
      "999 22.971927642822266\n",
      "1099 12.999251365661621\n",
      "1199 9.520439147949219\n",
      "1299 8.8980712890625\n",
      "1399 8.864032745361328\n",
      "1499 8.880899429321289\n",
      "1599 8.939823150634766\n",
      "1699 8.940942764282227\n",
      "1799 8.913397789001465\n",
      "1899 8.913410186767578\n",
      "1999 8.9237699508667\n",
      "Result: y = 0.0005278979078866541 + 0.8562362790107727 x +       0.0005281136254779994 x^2 + -0.09383614361286163 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "linear_layer = model[0]\n",
    "\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + \\\n",
    "      {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e08510",
   "metadata": {},
   "source": [
    "## 3.3 PyTorch: Custom nn Modules\n",
    "- 이미 존재하는 모델보다 더 복잡한 모델을 구현하고 싶다면 **사용자 정의 모델**을 구축해야 함\n",
    "- ```nn.Module``` 클래스를 상속하여 사용자 정의 모델을 구축할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6caec1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.a + self.b*x + self.c*x**2 + self.d*x**3\n",
    "    \n",
    "    def string(self):\n",
    "        return f\"y = {self.a.item()} + {self.b.item()}x + {self.c.item()}x^2 + {self.d.item()}x^3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae86a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1168.9569091796875\n",
      "199 792.3114624023438\n",
      "299 538.475341796875\n",
      "399 367.2430725097656\n",
      "499 251.62144470214844\n",
      "599 173.47265625\n",
      "699 120.59778594970703\n",
      "799 84.78569030761719\n",
      "899 60.50464630126953\n",
      "999 44.023948669433594\n",
      "1099 32.82545852661133\n",
      "1199 25.20763397216797\n",
      "1299 20.019813537597656\n",
      "1399 16.48282241821289\n",
      "1499 14.06859016418457\n",
      "1599 12.418822288513184\n",
      "1699 11.290163040161133\n",
      "1799 10.517105102539062\n",
      "1899 9.987018585205078\n",
      "1999 9.623120307922363\n",
      "Result: y = 0.023523803800344467 + 0.8395689129829407x + -0.004058246500790119x^2 + -0.09088782966136932x^3\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = Polynomial3()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f\"Result: {model.string()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ed308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
